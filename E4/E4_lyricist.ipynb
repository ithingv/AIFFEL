{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration 4\n",
    "\n",
    "### ë©‹ì§„ ì‘ì‚¬ê°€ ë§Œë“¤ê¸° ğŸµ\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [ë°ì´í„° ë¡œë“œ]()\n",
    "2. [ë°ì´í„° ì „ì²˜ë¦¬]()\n",
    "   - [ì •ê·œí‘œí˜„ì‹]()\n",
    "   - [í† í°í™”]()\n",
    "3. [í›ˆë ¨, í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬]()\n",
    "4. [ëª¨ë¸ ì„¤ê³„ ë° í›ˆë ¨]()\n",
    "5. [ëª¨ë¸ í‰ê°€]()\n",
    "6. [íšŒê³ ]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face [Verse 1]', \"Somethin' ain't right when we talkin'\", \"Somethin' ain't right when we talkin'\", \"Look like you hidin' your problems\", 'Really you never was solid', 'No, you can\\'t \"son\" me', \"You won't never get to run me\", 'Just when shit look out of reach', 'I reach back like one, three', 'Like one, three, yeah [Pre-Hook]', \"That's when they smile in my face\", 'Whole time they wanna take my place']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "raw_corpus = []\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpusì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "### 2-1. ì •ê·œí‘œí˜„ì‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>  <end>\n"
     ]
    }
   ],
   "source": [
    "# ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë¬¸ì¥ ì •ì œí•˜ëŠ” í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ì–‘ìª½ ê³µë°±ì„ ì‚­ì œ\n",
    "  \n",
    "    # ì•„ë˜ ë‹¨ê³„ë¥¼ ê±°ì³ sentenceëŠ” ìŠ¤í˜ì´ìŠ¤ 1ê°œë¥¼ delimeterë¡œ í•˜ëŠ” ì†Œë¬¸ì ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë°”ë€ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r'\\[[^)]*\\]', '', sentence)         # [Hook]ì²˜ëŸ¼ ëŒ€ê´„í˜¸ë¡œ íŒŒíŠ¸ êµ¬ë¶„í•˜ëŠ” ë¬¸ì ì œê±°\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)    # íŒ¨í„´ì˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ë§Œë‚˜ë©´ íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)           # ê³µë°± íŒ¨í„´ì„ ë§Œë‚˜ë©´ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence)  # a-zA-Z?.!,Â¿ íŒ¨í„´ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì(ê³µë°±ë¬¸ìê¹Œì§€ë„)ë¥¼ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # ë¬¸ì¥ ì•ë’¤ì— <start>ì™€ <end> ì¶”ê°€\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(raw_corpus[0]))   # ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> somethin ain t right when we talkin <end>',\n",
       " '<start> somethin ain t right when we talkin <end>',\n",
       " '<start> look like you hidin your problems <end>',\n",
       " '<start> really you never was solid <end>',\n",
       " '<start> no , you can t son me <end>',\n",
       " '<start> you won t never get to run me <end>',\n",
       " '<start> just when shit look out of reach <end>',\n",
       " '<start> i reach back like one , three <end>',\n",
       " '<start> like one , three , yeah <end>',\n",
       " '<start> that s when they smile in my face <end>',\n",
       " '<start> whole time they wanna take my place <end>',\n",
       " '<start> whole time they wanna take my place <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì •ì œëœ corpus ìƒì„±\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0 or len(sentence.split()) > 12: continue   # ê¸¸ì´ê°€ 0ì´ê±°ë‚˜ ë‹¨ì–´ê°€ 12ê°œ ë„˜ëŠ” ê¸´ ë¬¸ì¥ì€ ì œì™¸\n",
    "    if preprocess_sentence(sentence) == '<start>  <end>': continue  # ê³µë°±ë§Œ ìˆëŠ” ë¬¸ì¥ ì œì™¸\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "print(len(corpus))\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    # í…ì„œí”Œë¡œìš°ì—ì„œ ì œê³µí•˜ëŠ” Tokenizer íŒ¨í‚¤ì§€ë¥¼ ìƒì„±\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,   # ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜ \n",
    "        filters=' ',       # ë³„ë„ë¡œ ì „ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, ì‚¬ì „ì— ì—†ì—ˆë˜ ë‹¨ì–´ëŠ” ì–´ë–¤ í† í°ìœ¼ë¡œ ëŒ€ì²´í• ì§€\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # ìš°ë¦¬ê°€ êµ¬ì¶•í•œ corpusë¡œë¶€í„° Tokenizerê°€ ì‚¬ì „ì„ ìë™êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "    # ì´í›„ tokenizerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥í•  ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizerëŠ” êµ¬ì¶•í•œ ì‚¬ì „ìœ¼ë¡œë¶€í„° corpusë¥¼ í•´ì„í•´ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # paddingìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶”ê¸° (maxlen 15ë¡œ ì„¤ì •) \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    5   91  103   59   31  166    4   11  133   24   29   10   12\n",
      "     3]\n",
      " [   2   42  133   29   10   12    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    5   40  828  172 2394  828   37   10   12    3    0    0    0\n",
      "     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(158297, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë³€í™˜ëœ í…ì„œ í™•ì¸\n",
    "print(tensor[:3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì–´ì‚¬ì „ í™•ì¸\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   5  91 103  59  31 166   4  11 133  24  29  10  12]\n",
      "[  5  91 103  59  31 166   4  11 133  24  29  10  12   3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(158297, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source, target ë¬¸ì¥ ìƒì„±\n",
    "src_input = tensor[:, :-1]   # tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    # tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "src_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í›ˆë ¨, í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (126637, 14)\n",
      "Target Train: (126637, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tensorë¥¼ train, test ë°ì´í„°ë¡œ ë¶„ë¦¬\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ì„¤ê³„ ë° í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 1024)        5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 1024)        8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 12001)       12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.num_words + 1  # ë‹¨ì–´ì‚¬ì „ì˜ ë‹¨ì–´ ê°œìˆ˜ + 0:<pad>\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_size))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3958/3958 [==============================] - 140s 35ms/step - loss: 3.1474\n",
      "Epoch 2/10\n",
      "3958/3958 [==============================] - 142s 36ms/step - loss: 2.7386\n",
      "Epoch 3/10\n",
      "3958/3958 [==============================] - 140s 35ms/step - loss: 2.4893\n",
      "Epoch 4/10\n",
      "3958/3958 [==============================] - 141s 36ms/step - loss: 2.2553\n",
      "Epoch 5/10\n",
      "3958/3958 [==============================] - 144s 36ms/step - loss: 2.0483\n",
      "Epoch 6/10\n",
      "3958/3958 [==============================] - 147s 37ms/step - loss: 1.8670\n",
      "Epoch 7/10\n",
      "3958/3958 [==============================] - 143s 36ms/step - loss: 1.7079\n",
      "Epoch 8/10\n",
      "3958/3958 [==============================] - 138s 35ms/step - loss: 1.5666\n",
      "Epoch 9/10\n",
      "3958/3958 [==============================] - 146s 37ms/step - loss: 1.4445\n",
      "Epoch 10/10\n",
      "3958/3958 [==============================] - 147s 37ms/step - loss: 1.3400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96a878b610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ ìƒì„± í•¨ìˆ˜\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ ì¼ë‹¨ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ìƒì„±í• ë•ŒëŠ” ë£¨í”„ë¥¼ ëŒë©´ì„œ ë‹¨ì–´ í•˜ë‚˜ì”© ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # ìš°ë¦¬ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë°”ë¡œ ìƒˆë¡­ê²Œ ìƒì„±í•œ ë‹¨ì–´ê°€ ë©ë‹ˆë‹¤. \n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ ìƒˆë¡­ê²Œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì…ë ¥ ë¬¸ì¥ì˜ ë’¤ì— ë¶™ì—¬ ì¤ë‹ˆë‹¤. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´  while ë£¨í”„ë¥¼ ë˜ ëŒë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # ìƒì„±ëœ tensor ì•ˆì— ìˆëŠ” word indexë¥¼ tokenizer.index_word ì‚¬ì „ì„ í†µí•´ ì‹¤ì œ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # ì´ê²ƒì´ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±í•œ ìì—°ì–´ ë¬¸ì¥ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì—ê²Œ ì‘ë¬¸ ì‹œì¼œë³´ê¸°\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i was born to make you happy <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i was\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i hate you because i love u <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i hate\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the one so i make sure i behave <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> happy birthday , happy birthday , happy birthday woo , shake ! <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> happy\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. íšŒê³ \n",
    "-  í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í–ˆì§€ë§Œ ëšœë ·í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ì§€ ëª»í–ˆê³  í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ ë°©ë²•ì— ëŒ€í•´ ì¶”ê°€ì ì¸ ê³µë¶€ê°€ í•„ìš”í•  ê²ƒ ê°™ë‹¤.\n",
    "\n",
    "\n",
    "- ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì •ê·œí‘œí˜„ì‹ì„ ì˜ í™œìš©í•˜ëŠ” ê²ƒì€ ì•„ì£¼ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ê³µë¶€í–ˆê³  tensorflow APIì˜ Tokenizerë¥¼ ì‚¬ìš©í•´ ì–´ë–»ê²Œ ì „ì²˜ë¦¬í•´ì„œ Corpusë¥¼ ìƒì„±í•˜ëŠ”ì§€ ë°°ìš¸ ìˆ˜ ìˆì—ˆë‹¤. \n",
    "\n",
    "\n",
    "- ë£¨ë¸Œë¦­ ì§€í‘œë¥¼ ë§ì¶”ê¸° ìœ„í•´ ê°„ëµí•œ ì „ì²˜ë¦¬, embedding size, hidden size ë“±ì˜ ì¡°ì‘ì„ í•˜ì˜€ìœ¼ë‚˜ ê·¸ ê³¼ì •ì—ì„œ ì¤‘ë³µëœ ë‹¨ì–´/ë¬¸ì¥ ì œì™¸, batch_size, embbedding size, hidden sizeë¥¼ ì–´ë–»ê²Œ ì¡°ì •í• ì§€ ê³ ë¯¼í•´ë´ì•¼ í•  ê²ƒ ê°™ë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
